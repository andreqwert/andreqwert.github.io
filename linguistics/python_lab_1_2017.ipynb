{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Лабораторная 1. Элементарная обработка текста\n",
    "Введение в python.  \n",
    "Осень 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Подпись**: *Асланов Андрей Борисович, МГТУ им. Н.Э. Баумана, ИУ9*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной работе вам будет предложено провести элементарную обработку текста. Для работы понадобится [Полный корпус Шекспировских сочинений](http://norvig.com/ngrams/shakespeare.txt). Скачайте его в ту же папку, где находится файл с этим ipython notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Токенизация слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length (in symbols) = 4538521\n"
     ]
    }
   ],
   "source": [
    "# Читаем шекспировский корпус\n",
    "corpus_name = 'shakespeare.txt'\n",
    "with open(corpus_name) as fin:\n",
    "    corpus = fin.read()\n",
    "print('Corpus length (in symbols) = {}'.format(len(corpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала просто разобьем текст на слова. Словом мы называем последовательность латинских символов, разделенных чем-то кроме букв. Попробуем стандартный питоновский **split**. Разделите текст по пробелам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length (in words) = 980637\n",
      "['A', \"MIDSUMMER-NIGHT'S\", 'DREAM', 'Now', ',', 'fair', 'Hippolyta', ',', 'our', 'nuptial', 'hour', 'Draws', 'on', 'apace', ':', 'four', 'happy', 'days', 'bring', 'in', 'Another', 'moon', ';', 'but', 'O', '!', 'methinks', 'how', 'slow', 'This', 'old', 'moon', 'wanes', ';', 'she', 'lingers', 'my', 'desires', ',', 'Like', 'to', 'a', 'step', 'dame', ',', 'or', 'a', 'dowager', 'Long', 'withering', 'out', 'a', 'young', \"man's\", 'revenue', '.', 'Four', 'days', 'will', 'quickly', 'steep', 'themselves', 'in', 'night', ';', 'Four', 'nights', 'will', 'quickly', 'dream', 'away', 'the', 'time', ';', 'And', 'then', 'the', 'moon', ',', 'like', 'to', 'a', 'silver', 'bow', 'New-bent', 'in', 'heaven', ',', 'shall', 'behold', 'the', 'night', 'Of', 'our', 'solemnities', '.', 'Go', ',', 'Philostrate', ',']\n"
     ]
    }
   ],
   "source": [
    "# Разбивает корпус на слова с помощью функции split().\n",
    "words = corpus.split()\n",
    "print('Corpus length (in words) = {}'.format(len(words)))\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, сюда попали и все знаки препинания. \n",
    "К сожалению, **split** не удобен тем, что он не может разделять по нескольким символам. Здесь на помощь нам приходит **re.split**.\n",
    "\n",
    "**re** - это библиотека для работы с регулярными выражениями. В следующем задании мы познакомимся с ней поближе.\n",
    "А пока измените фрагмент кода так, чтобы он разбивал текст по _пробельным символам и знакам препинания_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length (in words) = 1236776\n",
      "['A', 'MIDSUMMER', 'NIGHT', 'S', 'DREAM', '', 'Now', '', '', 'fair', 'Hippolyta', '', '', 'our', 'nuptial', 'hour', '', 'Draws', 'on', 'apace', '', '', 'four', 'happy', 'days', 'bring', 'in', '', 'Another', 'moon', '', '', 'but', 'O', '', '', 'methinks', 'how', 'slow', '', 'This', 'old', 'moon', 'wanes', '', '', 'she', 'lingers', 'my', 'desires']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Модифицируем вызов, чтобы делить на слова по символам ' ', '\\n', '.', ',', '?', '!', ':', \"'\", '\"'\n",
    "words = re.split(\"[ \\n.,\\[\\]?!:&-;'\\\"]\", corpus)\n",
    "\n",
    "print('Corpus length (in words) = {}'.format(len(words)))\n",
    "print(words[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "От знаков препинания мы избавились, но вместо этого появились пустые строки (постарайтесь понять, почему они возникли). К счастью, мы без труда сможем их отфильтровать. Заодно избавимся от слов, содержащих символы, отличные от _a-z_ и _A-Z_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length (in words) = 838772\n",
      "['A', 'MIDSUMMER', 'NIGHT', 'S', 'DREAM', 'Now', 'fair', 'Hippolyta', 'our', 'nuptial', 'hour', 'Draws', 'on', 'apace', 'four', 'happy', 'days', 'bring', 'in', 'Another', 'moon', 'but', 'O', 'methinks', 'how', 'slow', 'This', 'old', 'moon', 'wanes', 'she', 'lingers', 'my', 'desires', 'Like', 'to', 'a', 'step', 'dame', 'or', 'a', 'dowager', 'Long', 'withering', 'out', 'a', 'young', 'man', 's', 'revenue']\n"
     ]
    }
   ],
   "source": [
    "# Избавимся от пустых слов и слов с неалфавитными символами\n",
    "\n",
    "words = list(filter(None, words)) # Избавимся от пустых слов\n",
    "#words = pass # Избавимся от слов с неалфавитными символами\n",
    "\n",
    "print('Corpus length (in words) = {}'.format(len(words)))\n",
    "print(words[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте отсортируем все слова, и посмотрим, что у нас получилось."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\n"
     ]
    }
   ],
   "source": [
    "# Now, let's sort all words in lexicographical order\n",
    "words.sort()\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не очень впечатляет, не правда ли? Время вспомнить про [defaultdict](https://docs.python.org/2/library/collections.html#collections.defaultdict). \n",
    "\n",
    "Добавьте слова в defaultdict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "\n",
    "word_counts = defaultdict(int)\n",
    "for word in words:\n",
    "    word_counts[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы напечатать самые часто встречаемые слова, напишем дополнительную функцию (аналог most_common у Counter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common(counts, n):\n",
    "    return Counter(counts).most_common(n)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 23301), ('I', 22084), ('and', 16890), ('to', 16110), ('of', 15048), ('you', 12543), ('a', 12430), ('my', 10701), ('in', 9584), ('d', 8606), ('is', 8410), ('that', 8269), ('not', 8012), ('me', 7735), ('s', 7261), ('And', 7222), ('it', 6899), ('be', 6430), ('with', 6199), ('your', 6132)]\n"
     ]
    }
   ],
   "source": [
    "# Используя определенную выше функцию, напечатаем 20 самых частых слов\n",
    "print(most_common(word_counts, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<font color='red'>Ответьте на следующие вопросы (внутри ipython ноутбука):</font>\n",
    "\n",
    "**Q**: Откуда взялись 'd' и 's'? К чему они относятся?\n",
    "\n",
    "**A**: Возможно, 'd' - сокращение от had/should/would/прочего при записи, а 's' - указывает на принадлежность. По типу \"sister's pen'\n",
    "\n",
    "**Q**: Предложите способы борьбы с подобными 'd' и 's'.\n",
    "\n",
    "**A**: Не учитывать их при поиске самых часто встречающихся слов в тексте"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но, как мы видим, все по прежнему не идеально. Например 'And' и 'and' являются разными словами. Приведите все к нижнему регистру и пересчитайте слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word_lower_counts:\n",
      " [('the', 26856), ('and', 24116), ('i', 22412), ('to', 19225), ('of', 16018), ('you', 14097), ('a', 13986), ('my', 12283), ('that', 11171), ('in', 10640)]\n"
     ]
    }
   ],
   "source": [
    "# напечатайте 10 самых частотных слов вместе с их счётчиками после приведения к нижнему регистру\n",
    "words_lower = [every_word.lower() for every_word in words]\n",
    "word_lower_counts = most_common(words_lower, 10)\n",
    "\n",
    "print('Word_lower_counts:\\n', word_lower_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<font color='red'>Ответьте на следующие вопросы (внутри ipython ноутбука):</font>\n",
    "\n",
    "**Q**: Приведите пример, когда два совершенно различных слова в итоге переходят в одно при приведении всего текста к нижнему регистру.\n",
    "\n",
    "**A**: Может, что-то вроде MosForum --> mosforum\n",
    "\n",
    "**Q**: Какие еще возможные проблемы при разделении текста на слова мы не учли?\n",
    "\n",
    "**A**: Пусть есть некоторое одиночное слово, написанное через дефис. При применении подобного способа такое слово будет разделено на два, что есть неверно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы хотим быстро проанализировать текст (например, определить его тему), нам может быть достаточно посмотреть на список самых популярных слов в нем. К сожалению, почти в любом тексте самыми популярными словами являются частицы, предлоги, местоимения и прочие малоинформативные слова. Их обычно называют [стоп-слова](https://ru.wikipedia.org/wiki/%D0%A8%D1%83%D0%BC%D0%BE%D0%B2%D1%8B%D0%B5_%D1%81%D0%BB%D0%BE%D0%B2%D0%B0).\n",
    "\n",
    "Сохраните [файл, содержащий стоп-слова](https://sites.google.com/site/kevinbouge/stopwords-lists/stopwords_en.txt?attredirects=0&d=1) в папку, где находится этот ipython notebook. Отфильтруйте слова из текста, используя слова из файла."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in stopset:  571\n",
      "['a', \"a's\", 'able', 'about', 'above', 'according', 'accordingly', 'across', 'actually', 'after'] \n",
      "\n",
      "\n",
      "\n",
      "Different words with lowering & without stopwords: 333906\n",
      "Most popular words in text (excluding stopwords): \n",
      "[('thou', 5443), ('thy', 3812), ('thee', 3104), ('good', 2888), ('lord', 2747), ('sir', 2543), ('ll', 2480), ('love', 2010), ('man', 1987), ('hath', 1917)]\n"
     ]
    }
   ],
   "source": [
    "# Прочитайте стоп-слова 'stopwords_en.txt'\n",
    "with open('stopwords_en.txt', \"r\") as stopwords:\n",
    "    stopwords = [row.strip() for row in stopwords] #strip удаляет лишние знаки, в данном случае перенос \\n\n",
    "\n",
    "print('Words in stopset: ', len(stopwords))\n",
    "print(list(stopwords)[:10], '\\n\\n\\n')\n",
    "\n",
    "\n",
    "# Отфильтруйте стоп-словa с использованием stopset\n",
    "word_lower_stop_counts = [word for word in words_lower if word not in stopwords]\n",
    "\n",
    "\n",
    "print('Different words with lowering & without stopwords:', len(word_lower_stop_counts))\n",
    "print('Most popular words in text (excluding stopwords): ')\n",
    "print(most_common(word_lower_stop_counts, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь список выглядит намного лучше. По первым 50 словам уже можно угадать автора текста."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И наконец, последняя задача. \n",
    "\n",
    "Нередко для объектов нам нужно найти их частоты в тексте, а не количество раз, которые они встречались.\n",
    "Для этого напишем функцию, которая превращает все значения _defaultdict_ в частоты (новый словарь может содержать дробные числа)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "counts = {}\n",
    "for word in word_lower_stop_counts:\n",
    "    counts[word] = counts.get(word, 0) + 1\n",
    "\n",
    "\n",
    "def normalize(counts):\n",
    "    \"\"\" \n",
    "    Преобразует абсолютные частоты в относительные. Возвращает новый словарь, исходный не изменяется.\n",
    "    \"\"\"\n",
    "    keys = [key for key in counts.keys()]\n",
    "    freqs = [\"%.7f\" % (count_value/sum(counts.values())) for count_value in counts.values()]\n",
    "    freqs_counts = dict(zip(keys, freqs))\n",
    "    return freqs_counts\n",
    "    \n",
    "\n",
    "freqs_counts = normalize(counts)\n",
    "#pp.pprint(freqs_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте с помощью нашей функции посмотрим на распределение частот символов в тексте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'a': '0.0761751',\n",
      "    'b': '0.0162639',\n",
      "    'c': '0.0299003',\n",
      "    'd': '0.0440961',\n",
      "    'e': '0.1243770',\n",
      "    'f': '0.0193705',\n",
      "    'g': '0.0271788',\n",
      "    'h': '0.0426548',\n",
      "    'i': '0.0610936',\n",
      "    'j': '0.0019398',\n",
      "    'k': '0.0117249',\n",
      "    'l': '0.0518949',\n",
      "    'm': '0.0268911',\n",
      "    'n': '0.0625858',\n",
      "    'o': '0.0689589',\n",
      "    'p': '0.0240218',\n",
      "    'q': '0.0014612',\n",
      "    'r': '0.0784358',\n",
      "    's': '0.0739112',\n",
      "    't': '0.0734248',\n",
      "    'u': '0.0333494',\n",
      "    'v': '0.0124270',\n",
      "    'w': '0.0169261',\n",
      "    'x': '0.0012111',\n",
      "    'y': '0.0191514',\n",
      "    'z': '0.0005748'}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# напишите функцию make_letter_counts(words), создающую defaultdict, в котором для каждого символа хранится его частота\n",
    "\n",
    "def make_letter_counts(words):\n",
    "    letter_counts = {}\n",
    "    for word in words:\n",
    "        for letter in word:\n",
    "            letter_counts[letter] = letter_counts.get(letter, 0) + 1\n",
    "    return letter_counts\n",
    "    \n",
    "\n",
    "letter_counts = make_letter_counts(word_lower_stop_counts)\n",
    "letter_freqs = normalize(letter_counts)\n",
    "pp.pprint(letter_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Частотное распределение букв может быть полезно для разных задач. Например, по нему можно распознать язык неизвестного текста с хорошей точностью. Про то, как изучать частоты более сложных объектов, мы поговорим через несколько занятий."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
