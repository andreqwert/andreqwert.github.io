{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание №13\n",
    "\n",
    "Асланов А.Б., ИУ9-21М\n",
    "\n",
    "Есть три текста:    \n",
    "$d_0: w_0, w_1, w_1$;    \n",
    "$d_1: w_0, w_1, w_2$;   \n",
    "$d_2: w_0, w_2, w_2$.    \n",
    "Сделать 5 итераций (проходов по трём текстам) EM-алгоритма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "После 5 операций:\n",
      "\n",
      "PHI (матрица слов в темах):\n",
      "\n",
      "                       topic_0   topic_1\n",
      "(@default_class, w2)  0.000334  0.665316\n",
      "(@default_class, w1)  0.665499  0.002181\n",
      "(@default_class, w0)  0.334167  0.332502\n",
      "\n",
      "\n",
      "\n",
      "THETA (матрица тем в документах):\n",
      "\n",
      "                0         1        2\n",
      "topic_0  0.999991  0.497708  0.00001\n",
      "topic_1  0.000009  0.502292  0.99999\n"
     ]
    }
   ],
   "source": [
    "import artm\n",
    "\n",
    "\"\"\"\n",
    "Для работы с BigARTM нужно сначала перевести текст в формат матрицы вхождений слов в документы (bag_of_words), \n",
    "а затем его в необходимый для работы библиотеки формат - батчи.\n",
    "Для того чтобы проделать это, используется конструктор BatchVectorizer, но он принимает на вход только формат\n",
    "вида библиотеки Vowpal Wabbit. \n",
    "Потом предварительно надо привести весь текст в формат Vowpal Wabbit:\n",
    "|text w0 w1 w1\n",
    "|text w0 w1 w2\n",
    "|text w0 w2 w2\n",
    "\n",
    "Далее полученный конструктор подается на вход алгоритма обучения (метод fit_offline).\n",
    "fit_offline на каждом проходе обновляет матрицы phi и theta. Используется для маленьких коллекций.\n",
    "\n",
    "cache_theta = флаг, позволяющий либо запрещающий хранить матрицу theta.\n",
    "Нужен по причине того, что матрица theta может занимать слишком много места (для больших коллекций),\n",
    "что часто неприемлемо. \n",
    "\n",
    "На выходе: матрицы слов в теме и матрица тем в документе.\n",
    "\"\"\"\n",
    "\n",
    "# подготовка данных\n",
    "batch_vectorizer = artm.BatchVectorizer(data_path='/Users/user/Desktop/em/doc.txt', data_format='vowpal_wabbit', target_folder='batches')\n",
    "T = 2 # количество тем\n",
    "iter_num = 5 # количество итераций\n",
    "dictionary = batch_vectorizer.dictionary\n",
    "\n",
    "model = artm.ARTM(num_topics=T, dictionary=dictionary, cache_theta=True)\n",
    "model.fit_offline(batch_vectorizer=batch_vectorizer, num_collection_passes=iter_num)\n",
    "phi = model.get_phi()\n",
    "theta = model.get_theta()\n",
    "print('После ' + str(iter_num) + ' операций:\\n')\n",
    "print('PHI (матрица слов в темах):\\n')\n",
    "print(phi)\n",
    "print('\\n\\n')\n",
    "print('THETA (матрица тем в документах):\\n')\n",
    "print(theta)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вывод\n",
    "\n",
    "После 5 итераций.    \n",
    "Для матрицы слов в теме:      \n",
    "1) К теме $t_0$ с наибольшей вероятностью относится слово $w_1$;      \n",
    "2) К теме $t_1$ с наибольшей вероятностью относится слово $w_2$.      \n",
    "\n",
    "Для матрицы тем в документе:     \n",
    "1) К теме $t_0$ с наибольшей вероятностью относится документ $d_0$;     \n",
    "1) К теме $t_1$ с наибольшей вероятностью относится документ $d_2$;    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
